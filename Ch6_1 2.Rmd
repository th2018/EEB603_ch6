---
title: "Chapter 6"
subtitle: 'Part A: Importing Data Sets from the Internet'
author: "Tao Huang & Kara Navock"
date: "`r Sys.Date()`" #YAML
output: 
  pdf_document:
    toc: yes
urlcolor: blue

---

\newpage

# Learning outcomes
(1) Know how to import data into R.
(2) Be able to explain web scraping.
(3) Be able to download data through APIs.

Files are as follows:

- \verb|Ch6_1 2.Rmd|: This is the \verb|.Rmd| file used to compile the pdf of this class. 

1. Import structured data
2. Import unstructured data through APIs and packages
3. Import unstructured data through web scraping

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, echo=FALSE, warning=FALSE, include=FALSE}
#install and load the packages
installed<-as.data.frame(installed.packages())
for (p in c("knitr","rmarkdown","formattable","kableExtra","dplyr","magrittr","prettydoc","htmltools","knitcitations"))
  {
    if (p %in% installed$Package)
    {
    library(p,character.only = T)
    } 
      else
      {
      install.packages(p)
      }
}

#Generate BibTex citation file for all R packages used to produce report
knitr::write_bib(.packages(), file = 'packages.bib')
```

setwd()

Data format
.Rdata: The best way to store objects from R is with .RData files.

A section introducing the data (dataset) used to support these exercises 

# 6.3 Importing Data Sets from the Internet
### (a) Before you can analyze and visualize data, you have to get that data into R.
### (b) read.csv function is commonly used.
### (c) read.csv("a filepath or an URL")
## 6.3.1 Data from non-secure (http) URLs
### What is a URL?
### A URL can be typed into your browser's address bar.
### import with read.csv
```{r , echo=TRUE, results="hide" }


#Open a connection.

url("https://raw.githubusercontent.com/karanavock/Rep_Sci/master/DB8.csv")

#If you see "r" mode, you can read it

url("https://raw.githubusercontent.com/karanavock/Rep_Sci/master/DB8.csv")
  #Modes: "r"(read), "w"(write)
#import the URL
URL_data<-read.csv("https://raw.githubusercontent.com/karanavock/Rep_Sci/master/DB8.csv")
# what kind of object is it?
class(URL_data) # A data frame is a table
#The data from the URL is now in a data.frame now.
#check the head of the data
head(URL_data)
summary(URL_data)
URL_complete<-URL_data[complete.cases(URL_data),]
```



```{r URL_data, echo=FALSE, results="hide", fig.cap="Plot of width in relation to height.", out.width = '100%'}
plot(URL_data$Height,URL_data$Width,xlab = "Height",ylab="Width")
```

## 6.3.1 Data from non-secure (http) URLs
### import with read.table
```{r}
URL_data_2<-read.table("https://raw.githubusercontent.com/karanavock/Rep_Sci/master/DB8.csv")

#header: a logical value indicating whether the file contains the names of the variables as its first line. 
#sep: the field separator character.

head(URL_data_2)
```

download.file is used to download a file from the Internet.
Example: download.file from https://www.neonscience.org/data/about-data/spatial-data-maps using the neonUtilities R package
The neonUtilities R package is designed specifically to access the NEON API.
```{r}
#download.file(url, destfile)
#download.file("https://www.neonscience.org/sites/default/files/NEONAquaticWatershed.zip",destfile="/Users/owner/Desktop/NEONAquaticWatershed.zip")
#setwd()
url="https://www.neonscience.org/sites/default/files/NEONAquaticWatershed.zip"
destfile="NEONAquaticWatershed.zip"
download.file(url, destfile)


```

```{r}
download.file("https://mikethetesternz.files.wordpress.com/2019/02/apinotipa.png",destfile="IPA2.png")

```

## download.file
```{r}
#download.file("https://cshperspectives.cshlp.org/content/8/9/a023218.full.pdf",destfile="/Users/owner/Desktop/EEB603/Ch6/paper.pdf")
```

# 6.4 Advanced Automatic Data Gathering: Web Scraping

Data retrieval from internet: (1) web scraping or (2) web APIs.
Web scraping is the process of collecting and parsing raw data from the Web.
Collecting data from websites using an automated process is known as web scraping. 
HTML (Right-click the page and click on “View Page Source,”)
To extract the HTML from the page.
Some websites have web APIs.

#Pseudocode for Web Scraping 
#Example:export number of papers of each species on Google Scholar
```{r}
#creat a lisf of birds
#for (bird in bird_list)
  #{
  #enter the bird into the search bar
  #hit search
  #export the number of papers
  #}
```

Skills you need: HTML and looping

# 6.3.4 Data APIs & feeds

APIs present researchers with a diverse set of data sources through a standardised access mechanism: send a pasted together HTTP request, receive JSON or XML in return.

In response to an API request, the API will return an HTTP response that includes: (1) An HTTP status code. (2) Headers, key-value pairs. (3) A body typically consisting of XML, JSON, plain text, HTML, or some kind of binary representation.

We need to parse the output into an R object.

Some packages API:
rnoaa

```{r Fig_1, echo=FALSE, fig.cap="API Request (https://www.dataquest.io/blog/r-api-tutorial/)", out.width = '100%'}
knitr::include_graphics("/Users/owner/Desktop/EEB603/Ch6/api-request.png")
```


```{r Fig_2, echo=FALSE, fig.cap="API Request https://github.com/yusuzech/r-web-scraping-cheat-sheet", out.width = '100%'}
knitr::include_graphics("/Users/owner/Desktop/EEB603/Ch6/functions_and_classes.png")
```

The term API is an acronym, and it stands for Application Programming Interface.
APIs offer users a polished way to request clean and curated data from a website. 
To work with APIs in R, we need to bring in some libraries. 
### httr::GET (request to the server) 
The GET method means retrieve whatever information
### class(httr::GET("https://www.google.com/"))
```{r}
# install neonUtilities - can skip if already installed
#install.packages("neonUtilities")
# load neonUtilities
library(neonUtilities)
```

## The identifier of the NEON data product: https://data.neonscience.org/data-products/explore
```{r}
zipsByProduct(dpID="DP1.20093.001", #Chemical properties of surface water
              site="BIGC", #Upper Big Creek, CA
              startdate = "2019-01", enddate = "2019-04",
              check.size = FALSE #R would ask you to approve the file size
              #,package = "basic", avg = "all", savepath = NA,load = F
  )
```

##exercise
```{r}
#find Breeding landbird point counts (DP1.10003.001) in Dead Lake, AL (DELA) from 2016 to 2019 with the function zipsByProduct

```


# To sum up
1. What function can you use to read a csv from a URL?
### read.csv
2. What function can you use to download a file from a URL?
## download.file
#urls <- c("http://link1", "http://link2", "http://link3")
#for (url in urls) {download.file(url, destfile = basename(url))}
3. Which of the following statements about web scraping and web APIs are true:
- All websites have APIs.
- A web scraping script for Website A can be applied to Website B.
- A http request returns XML or JSON files.

# References
```{r echo=FALSE, include=FALSE}
#write.bibtex(file='packages.bib')
```

----
### https://www.neonscience.org/resources/data-tutorials?type=All&field_ds_tags_tid=All&field_ds_languages_tid=1215&page=7
#source_DropboxData
# DispropData <- repmis::source_data(UrlAddress)
